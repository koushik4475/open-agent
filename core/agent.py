# openagent/core/agent.py
"""
Agent Core â€” the central orchestrator.

Flow for every user message:
  1. Retrieve relevant memory (RAG context injection)
  2. Route the input â†’ pick the right tool
  3. Execute the tool (or fall back to raw LLM)
  4. Store the interaction in memory
  5. Return the response

Design decisions:
  - Memory retrieval happens BEFORE routing so context is always available.
  - Tools are async. The agent awaits each one.
  - If a tool fails, we fall back to the LLM with an error note in the prompt.
"""

from __future__ import annotations
import logging
from pathlib import Path

from openagent.config import settings
from openagent.core.llm import LLMClient
from openagent.core.router import route, ToolName
from openagent.memory.store import MemoryStore

# Tool implementations
from openagent.tools.offline.summarize import summarize_text
from openagent.tools.offline.run_command import run_sandboxed_command
from openagent.tools.online.web_search import web_search
from openagent.tools.online.web_fetch import web_fetch
from openagent.parsers.unified import parse_file

logger = logging.getLogger("openagent.agent")

# â”€â”€ System prompt injected into every LLM call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """You are OpenAgent, a powerful Hybrid AI assistant.
You combine local privacy with high-performance cloud intelligence.
You have access to real-time tools:
- Web Search & Fetch (active)
- File Parsing & OCR
- Command Execution & Summarization

IDENTITY:
You were developed by **Koushik HY**.
Developer Details: [https://koushikhy.netlify.app](https://koushikhy.netlify.app)
Always mention this when asked about your creator.

Always be honest and concise. Use your tools whenever needed to answer the user's request accurately."""


class Agent:
    def __init__(self, llm: LLMClient, memory: MemoryStore):
        self.llm = llm
        self.memory = memory
        self.cfg = settings

    @classmethod
    async def create(cls) -> "Agent":
        """Factory â€” initializes LLM client and memory store."""
        llm = LLMClient()
        memory = await MemoryStore.create()
        return cls(llm=llm, memory=memory)

    async def run(self, user_input: str, history: list[dict] | None = None) -> str:
        """
        Main entry point. Takes user text, returns agent response string.
        """
        history = history or []

        # â”€â”€ Step 1: Retrieve memory context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        memory_context = await self.memory.retrieve(user_input)

        # â”€â”€ Step 2: Route to a tool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tool_name, ctx = await route(user_input)
        logger.info(f"Routed to: {tool_name.value}")

        # â”€â”€ Step 3: Execute the selected tool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            response = await self._execute_tool(tool_name, ctx, memory_context, history)
        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            # Fallback: send to LLM with error context
            response = await self._llm_fallback(user_input, memory_context, error=str(e))

        # â”€â”€ Step 4: Store in memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await self.memory.store(user_input, response)

        return response

    # â”€â”€â”€ Tool dispatch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _execute_tool(
        self,
        tool: ToolName,
        ctx: dict,
        memory_ctx: str,
        history: list[dict],
    ) -> str:

        if tool == ToolName.PARSE_FILE:
            filepath = ctx.get("filepath")
            if not filepath:
                return "âš ï¸ No file path provided. Use: /file <path>"
            text = parse_file(Path(filepath))
            # After parsing, send to LLM for analysis
            prompt = self._build_prompt(
                ctx.get("prompt", "Analyze this file content."),
                memory_ctx,
                file_content=text,
            )
            return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

        elif tool == ToolName.OCR_IMAGE:
            filepath = ctx.get("filepath")
            if filepath:
                text = parse_file(Path(filepath))  # unified parser handles images
                prompt = self._build_prompt(
                    ctx.get("prompt", "What does this image contain?"),
                    memory_ctx,
                    file_content=text,
                )
                return await self.llm.generate(prompt, system=SYSTEM_PROMPT)
            return "âš ï¸ No image file provided. Use: /file <path_to_image>"

        elif tool == ToolName.SUMMARIZE:
            prompt = self._build_prompt(ctx["prompt"], memory_ctx)
            return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

        elif tool == ToolName.RUN_COMMAND:
            return await run_sandboxed_command(ctx["prompt"], self.llm)

        elif tool == ToolName.WEB_SEARCH:
            search_results = await web_search(ctx.get("query", ctx["prompt"]))
            prompt = self._build_prompt(
                ctx["prompt"],
                memory_ctx,
                web_results=search_results,
            )
            return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

        elif tool == ToolName.WEB_FETCH:
            url = ctx.get("url")
            if not url:
                return "âš ï¸ No URL found. Include a full URL (https://...) in your message."
            page_text = await web_fetch(url)
            prompt = self._build_prompt(
                ctx["prompt"],
                memory_ctx,
                web_content=page_text,
            )
            return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

        else:  # GENERAL
            offline_warning = ctx.get("offline_warning", "")
            prompt = self._build_prompt(ctx["prompt"], memory_ctx)
            if offline_warning:
                prompt = f"[NOTE: {offline_warning}]\n\n" + prompt
            return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

    # â”€â”€â”€ Prompt construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _build_prompt(
        user_query: str,
        memory_ctx: str = "",
        file_content: str = "",
        web_results: str = "",
        web_content: str = "",
    ) -> str:
        """
        Assembles the full prompt with all available context.
        Context sections are clearly labeled so the LLM knows what's what.
        """
        parts: list[str] = []

        if memory_ctx:
            parts.append(f"[PAST MEMORY CONTEXT]\n{memory_ctx}\n[END MEMORY]")

        if file_content:
            # Truncate very large files to avoid blowing the context window
            truncated = file_content[:8000]
            if len(file_content) > 8000:
                truncated += "\n... [content truncated for context limit]"
            parts.append(f"[FILE CONTENT]\n{truncated}\n[END FILE]")

        if web_results:
            parts.append(f"[WEB SEARCH RESULTS]\n{web_results}\n[END SEARCH]")

        if web_content:
            truncated = web_content[:6000]
            if len(web_content) > 6000:
                truncated += "\n... [page content truncated]"
            parts.append(f"[WEB PAGE CONTENT]\n{truncated}\n[END PAGE]")

        parts.append(f"[USER QUERY]\n{user_query}")

        return "\n\n".join(parts)

    # â”€â”€â”€ Fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _llm_fallback(self, user_input: str, memory_ctx: str, error: str) -> str:
        prompt = (
            f"[ERROR in tool execution: {error}]\n\n"
            f"Please answer the user's question as best you can.\n\n"
            f"{user_input}"
        )
        if memory_ctx:
            prompt = f"[MEMORY]\n{memory_ctx}\n[END MEMORY]\n\n" + prompt
        return await self.llm.generate(prompt, system=SYSTEM_PROMPT)

    # â”€â”€â”€ Utility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def print_tools(self):
        print("\n  ğŸ“¦ Available Tools:")
        print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("  ğŸ“„ parse_file     â†’ Parse TXT, PDF, DOCX files")
        print("  ğŸ–¼ï¸  ocr_image     â†’ Extract text from images (OCR)")
        print("  ğŸ“ summarize      â†’ Summarize or analyze text")
        print("  ğŸ’¬ general        â†’ General Q&A via local LLM")
        print("  ğŸ”§ run_command    â†’ Execute sandboxed shell commands")
        print("  ğŸŒ web_search     â†’ Search the web (requires internet)")
        print("  ğŸ”— web_fetch      â†’ Fetch and read a webpage (requires internet)")
        print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
